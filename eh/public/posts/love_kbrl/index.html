<!DOCTYPE html>
<html lang="en-us">
<head>
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script type="text/javascript" src="https://latest.cactus.chat/cactus.js"></script>
  <link rel="stylesheet" href="https://latest.cactus.chat/style.css" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> A Love Letter to Knowledge-Based Reinforcement Learning | Eisuke 英祐</title>
  <link rel = 'canonical' href = 'https://ei5uke.github.io/posts/love_kbrl/'>
  <meta name="description" content="Welcome welcome. In this blog, you can find updates on whatever I&#39;m currently doing at the moment. Have a fun stay :&gt;">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="A Love Letter to Knowledge-Based Reinforcement Learning" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ei5uke.github.io/posts/love_kbrl/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-01-29T00:00:00+00:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="A Love Letter to Knowledge-Based Reinforcement Learning"/>
<meta name="twitter:description" content=""/>

  
  
    
  
  
  <link rel="stylesheet" href="https://ei5uke.github.io/css/styles.7c754dee4bfc873743b12288cf94eadcfd3bee1f2cff5a106753b8047879ac5195c8d8ad90f6d73f12e9c841696c51a0abdeae8394e3eb3079f310f418ddf9ec.css" integrity="sha512-fHVN7kv8hzdDsSKIz5Tq3P077h8s/1oQZ1O4BHh5rFGVyNitkPbXPxLpyEFpbFGgq96ug5Tj6zB58xD0GN357A=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://ei5uke.github.io/images/favicon.ico" />

  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

    <header id="header">
  <a href="https://ei5uke.github.io/">
  
    <div id="logo" style="background-image: url(https://ei5uke.github.io/logos/kittykat.png)"></div>
  
  <div id="title">
    <h1>Eisuke 英祐</h1>
  </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#" aria-label="Menu"><i class="fas fa-bars fa-2x" aria-hidden="true"></i></a>
      </li>
      
        <li><a href="/about_me">about me</a></li>
      
    </ul>
  </div>
</header>



    
<link rel="icon" type="image/x-icon" href="/logos/favicon.ico">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<style>  
    iframe {
      float: center;
    }
    img {
      float: center;
    }
</style>
<body>
    <p style="text-align: center;">
        &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 
    </p>
    Dear Knowledge-Based Reinforcement Learning,
    <p style="text-indent: 40px">
        What even are you? Like, what's the difference between you and Reinforcement Learning? WHAT EVEN IS "KNOWLEDGE-BASED"?? Nevertheless, all the cool kids seem to like you (e.g. Sergey Levine). So, I'd like to get to know you better.
    </p>
    <p style="text-align: right;">
        Sincerely,
        <br> Eisuke
    </p>
    <p style="text-align: center;">
        &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 &#10084 
    </p>
    <br>
    All jokes aside, Knowledge-Based Reinforcement Learning (KBRL) is a topic is complicated to me.
    That being said, I plan to focus my undergraduate research on this subject, so I hope that I can explain to the audience
    my current understanding of KBRL. Throughout this page, I'll go over a few main things:
    <ul>
        <li>The definition of "Knowledge"</li>
        <li>KBRL vs (traditional) RL</li>
        <li>Current status</li>
    </ul>
    <p style="font-size:1.2vw;">&#10084 Definition of KNOWLEDGE</p> 
    <p>
        <i>To preface, much of the vocabulary I'll be using in this writing will come from [8], which is a journal my advisor
        published. Also, I may be incorrect about some things, so if that's the case, please contact me and I'll try
        to fix it asap!</i>
    </p>
    <p>
        <b>Def. 1 (Knowledge)</b> "We use <b>declarative knowledge</b> to refer to such knowledge represented as relational 
        statements. Many methods have been developed for reasoning with declarative knowledge (RDK), often using logics."
        <i>This declarative knowledge is what I'll be referring to as knowledge in this writing. </i>
    </p>
    <p style="text-align:center;">(Zhang and Sridharan, 2022)</p>
    <p>
        One way to think about this is 
        that knowledge is a truth regarding some aspect of the world. For example, the weight of an object can be considered
        as knowledge. Furthermore, there exists various types of knowledge, commonsense knowledge being one of them. This exists
        as a statement that is "humanly" sound.
    </p>
    <p>
        <b>Example. 1 (commonsense knowledge)</b> "Books are usually in the library but cookbooks are in the kitchen." [8]
    </p>
    <p>
        In terms of KBRL, much of the literature incorporates knowledge using logical reasoning. One way we 
        can use logical reasoning is to leverage information about a certain history of state-action pairs. 
        For example, if the state of the agent implies that our agent has not 
        passed a certain door, then we can with certainty claim that the agent exists within a certain room. Using this
        outcome of the logical reasoning, we can apply it to train a policy.
        Thus, KBRL aims to utilize these certain truths in addition to prior RL ideas to aid an agent to discover the optimal
         policy more effectively.
    </p>
    <p style="font-size:1.2vw;">&#10084 KBRL vs (traditional) RL</p> 
    <p>
        One question people may have is what exactly the difference is between KBRL and RL. In theory, RL should only use the reward
        function defined by default. KBRL takes this one step further with the addition of knowledge.
    </p>
    <p>
        For instance, we can use knowledge to diversify rewards by formulating a task as an
        MDP+RM rather than a traditional MDP:
    </p>
    <p>
        <b>Def. 2 (Reward Machine)</b> Given a set of propositional symbols \(P\), a set of (environment) states \(S\), 
            and a set of actions \(A\), a reward machine \((RM)\) is a tuple \(R_{PSA}\) = \(\langle U, u_0, F, \delta_u, 
            \delta_r \rangle\) where \(U\) is a finite set of states, \(u_0 \in U\) is an initial state, \(F\) is a finite set of
            terminal states (where \(U \cap F = \emptyset\)), \(\delta_u\) is the state-transition function, \( \delta_u : 
            U\times 2^P \rightarrow U \cup F\), and \(\delta_r\) is the state-reward function, \(\delta_r: U \rightarrow
            [S \times A \times S \rightarrow \mathbb{R}]\).
    </p>
    <p style="text-align:center;">(Icarte et al., 2022)</p>
    <p>
        By implementing this additional automata, we can leverage truths of the state to incentivize certain behaviors.
        If the agent transitions from a certain RM state to the next, we can grant additional rewards, perhaps rewards exponentially
        larger than the base reward function the user created.
    </p>
    <p>
        That being said, similar ideas already exist within traditional RL papers that don't differentiate itself from KBRL.
        One strong example comes from [2] in their project of using the IsaacGym simulator for training locomotion
        policies for quadrupedal robots:
    </p>
    <p style="text-align:center;">
        \(
            \begin{array}{c c c}
            \hline
            & \text{definition} & \text{weight} \\
            \text{Linear velocity tracking} & \phi (v^{*}_{b,xy} - v_{b,xy}) & 1dt \\
            \text{Angular velocity tracking} & \phi (\omega^{*}_{b,z} - \omega_{b,z}) & 0.5dt \\
            \text{Linear velocity penalty} & -v^2_{b,z} & 4dt \\
            \text{Angular velocity penalty} & -||\omega_{b,xy}||^2 & 0.05dt \\
            \text{Joint motion} & -||\ddot q_{j}||^2 -||\dot q_{j}||^2 & 0.001dt \\
            \text{Joint torque} & -||\tau_{j}||^2 & 0.0002dt \\
            \text{Action rate} & -||\dot q^*_{j}||^2 & 0.25dt \\
            \text{Collisions} & -n_{collision} & 0.001dt \\
            \text{Feet air time} & \sum^4_{f=0} (t_{air,f}-0.5) & 2dt \\
            \end{array}
        \)
    </p>
    <p style="text-align:center;">(Rudin et al., 2022)</p>
    <p>
        This paper diversifies the rewards given to the agent by having a well-planned, extensive reward function. Many of these
        bonus rewards can be considered as knowledge. For instance, linear and angular velocity tracking considers the state
        of the robot at two consecutive timesteps. Furthermore, feet air time also must keep track of a history of states to
        determine the length of which a foot is aerial. These concepts inherintly enhance quadrupedal locomotion as the robot
        learns the fundamentals of what it means to walk "well".
    </p>
    <p>
        These facts posit the question: is there really any true difference between KBRL and RL?
    </p>
    <p style="font-size:1.2vw;">&#10084 Current Status</p> 
    <p>
        There isn't too much about <b>specifically </b> "KBRL" in the first place. Searching the key phrase 
        "knowledge-based reinforcement learning" on Arxiv only results in 2 preprints (as of writing this blog 
        on jan 26, 2023), and both of which are from late 2019 to early 2020. On the other hand, the phrase
        "reinforcement learning" + "knowledge" outputs with 1905 results (of course much of which don't relate
        to KBRL). Hence, it looks like KBRL is currently a phrase that the community uses to refer to the combination
        of RDK and RL, rather than some specifically defined terminology set in stone.
    </p>
    <p>
        The best compilation of sources I can find regarding <b>specifically</b> KBRL comes from the Knowledge Based 
        Reinforcement Learning Workshop at IJCAI-PRICAI 2020, Yokohama, Japan [3]. The conference has a five-hour 
        long video (half of which I don't understand the material) but I'll breifly touch on the ones I like 
        [4, 5, 6] and also understand:
    </p>
    <iframe width="705" height="400" src="https://ijcai20.org/w27/" title="Learning to Simultaneously Navigate and Manipulate with Deep Q-Learning for Mobile Rearrangement" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <p style="text-align:center;">(KBRL Workshop, 2022)</p>
    <p> <b>Reinforcement Learning with External Knowledge by using Logical Neural Networks.</b></p>
    <p>
        This paper introduces ways to incorporate Logical Neural Networks (LNNs) with RL. The authors develop two methods:
        LNNs-shielding and LNNs-Guide,
    </p>
    <div class="row">
        <div class="column">
            <img src=/img/lnns-shield.png style="float:left;width:50%"> 
          </div>
        <div class="column">
            <img src=/img/lnns-guide.png style="float:right;width:50%"> 
          </div>
    </div> 
    <p style="text-align:center;">(Kimura et al., 2021)</p>
    <p>
        which both aid in achieving Sample-Efficient RL. This branch of RL aims to develop policies that converge faster, 
        combatting against the need for tremendous amounts of data that traditional RL requires (for one of the projects I 
        work on, I had to collect almost 800 GBs of data!!). Both methods use logical reasoning to affect the actions
        taken by an agent. LNNs-Shielding prevents poor actions from being performed and LNNs-Guide offers recommendations
        to actions (which the authors observes achieves better than shielding due to the option of giving positive 
        recommendations).
    </p>
    <p> <b>COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning.</b></p>
    <p>
        The authors of this paper aim to incorporate commonsense knowledge into RL. Through using a dataset collected in the past,
        they hope to develop an agent that has common sense. Their example follows such:
    </p>
    <p>
        <b>Example. 2 (using commonsense knowledge)</b> Imagine a robot is in the kitchen and is tasked to cut an onion. 
        The robot must then find a knife, so using common sense, the robot will first check any drawers.
    </p>
    <p>
        In an implementation of traditional RL, the robot must repeat the action of opening a drawer many times to understand
        that it must open a drawer to <i>potentially</i> find a knife. The authors hope to avoid this path, given that 
        it doesn't closely represent human behavior. What makes more sense would be for a human to have already opened a drawer
        in the past, and then using that past experience as common sense, apply it to the current task. The authors observe that
        through their method, the trained policy is able to encounter tasks and situations never seen in history before, 
        while SOTA RL methods like SAC cannot.
    </p>
    <p> <b>Learning from Failure: Introducing Failure Ratio in Reinforcement Learning.</b></p>
    <p>
        I like this paper becaues in my mind, it's intuitively easy to follow. The authors introduce that in a game 
        setting (such as Chess, Go, 
        or Othello), players may encounter a situtation where they want to undo the last move. This fundamental idea is 
        what leads to their main novelty: the Failure Ratio.
    </p>
    <p style="text-align:center;">
        \(
            \begin{array}{c}
            U'(s_t, a_t) = U(s_t, a_t) + \gamma^{n_{ep}}\alpha f(s_t, a_t) \\
            f(s_t, a_t) = Q(s_t, a_t) - Q(s_{t+2}, a_{t+2})
            \end{array}
        \)
    </p>
    <p style="text-align:center;">(Narita and Kimura, 2022)</p>
    <p>
        This novelty updates the Monte Carlo Tree Search (MCTS) algorithm used in AlphaGo [9]. Normally, the agent expands 
        the tree by taking the maximum value predicted winning ratio which is biased by whether the agent has visited 
        this node in the past. The failure ratio adds another bias by incentivizing the agent to also visit nodes 
        where the expected performance is poor. Through this new layer of exploration, Narita and Kimura find that MCTS + 
        Failure Ratio (1) performs better than vanilla MCTS in the early stages of learning the game of Othello (2) 
        converges faster than vanilla MCTS to the optimal policy. 

    </p>
    
    <p>
        <b>A Comprehensive Survey on Safe Reinforcement Learning</b>
    </p>
    <p>
        Lastly, I also want to briefly touch upon [7], which wasn't part of the workshop. This is an extensive report on 
        everything related to Safe RL (as of 2015
        which is when this paper was published). This paper defines two ways to approach Safe RL: (1) "transforming the
        optimization criterion" (2) modifiying the exploration process by the "incorporation of external knowledge and 
        an error metric" (note. The first method inherently modifies the 
        exploration process). We can see that through this classification, Safe RL is also a category of
        KBRL and uses logical reasoning to change the learning process. This branch aims to prevent the most harm
        caused onto the agent or the world, so it's clear how this integrates well with robotics with hopes to preserve
        robot parts and incentivize robots from injuring bystanders. 
    </p>

    <p style="font-size:1.2vw;">&#10084 Overview</p> 
    <p>
        Knowledge-Based RL exists as a branch of RL that incorporates knowledge with RL. Generally, we can see that 
        KBRL tries to evolve RL into a more "human-like" approach. I find this concept fascinating and I think much of 
        the AI community would find interest in such due to the goal of reaching AGI. Of course, a mere sophomore-undergrad 
        isn't going to solve AGI, but I hope to gain more experience in these "human-like" concepts throughout my next few years,
        and I'll work on continuing this path during grad school!
    </p>
    
    <p style="font-size:1.2vw;">&#10084 References</p> 
    [1] Icarte et al. Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning. 2022. <br>
    [2] Rudin et al. Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning. 2022. <br>
    [3] KBRL Workshop. IJCAI-PRICAI. 2020. <br>
    [4] Kimura et al. Reinforcement Learning with External Knowledge by using Logical Neural Networks. 2021. <br>
    [5] Singh et al. COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning. 2020. <br>
    [6] Narita and Kimura. Learning from Failure: Introducing Failure Ratio in Reinforcement Learning. 2020. <br>
    [7] García and Fernández. A Comprehensive Survey on Safe Reinforcement Learning. 2015. <br>
    [8] Zhang and Sridharan. A survey of knowledge-based sequential decision-makingunder uncertainty. 2022. <br>
    [9] Silver et al. Mastering the game of Go with deep neural networks and tree search. 2016. <br>

</body>


    <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2023  Eisuke 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/about_me">about me</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
</html>
