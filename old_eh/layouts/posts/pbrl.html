{{ define "main" }}
<link rel="icon" type="image/x-icon" href="/logos/favicon.ico">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<style>  
    iframe {
      float: center;
    }
    img {
      float: center;
    }
</style>
<body>

    <p>
        <i style="color:gray;">
        Continuing off from my last post (about Knowledge-Based RL), this topic also revolves around "human knowledge" 
        to increase RL performance. 
        </i>
    </p>

    <p>
        For a quick reminder, in Knowledge-Based RL (KBRL), we leverage human knowledge to enhance an agent's performance. We accomplish
        this in numerous ways, but research most commonly uses logical reasoning (e.g. Logical Neural Networks vs. Markov Networks,
        First Order Logic, Graphs, etc.) to visualize human knowledge and the relationship between various entities, and an RL 
        component exploits such information. 
    </p>

    <p>
        From such definition, KBRL stands to use <i>facts</i>, or at least <i>commonsense</i>, to obtain higher feats. Generally, 
        these two ideas can be <i>explainable</i> (a word that one of my labmates really dislikes lol -> how do you define explainability?).
        Then, <b>what about information that have low explainability</b>? For example, I really like vanilla ice cream over chocolate
        ice cream. Why? I just <i>prefer</i> it.
    </p>

    <figure style="display:block; margin: auto; width:25%">
        <img src={{.Params.vanilla}} style="display:block; margin: auto; width:100%" > 
        <figcaption>mmm. vanilla is very yummy in my stummy.</figcaption>
    </figure> 

    <p>
        This concept forms the basis for <b>Preference-Based Reinforcement Learning</b> (PBRL). In essence, we know that humans
        prefer A over B, even when there's no clear-cut explanation. Following suite, we apply this to creating better policies. 
    </p>

    <p>
        In this blog, I'll briefly explain Preference-Based RL and go over the following:
    </p>
    <ul>
        <li style="color:#C495C3">Intro to PBRL & technical explanation</li>
        <li style="color:#7E91EF">Current PBRL advancements</li>
        <li style="color:#ee4949">PBRL for Language Models (!!)</li>
    </ul>

    <p style="font-size:1.2vw; color:#C495C3">&#13145 Intro to PBRL & technical explanation</p> 
    <p>
        <span style="color:#C495C3">&#5463</span> So what is PBRL and in what cases might PBRL be advantageous compared 
        to normal RL? <span style="color:#C495C3">&#5465</span>
    </p>
    <p>
        To preface, Christiano et al. [6] explain how defining a great reward function may be challenging in some tasks. 
        The authors state:
    </p>
    <blockquote style="padding: 15px; background: #d7cccc; border-radius: 5px;"><i>
        "For example, suppose that we wanted to use reinforcement learning to train a robot to clean a table or
        scramble an egg. It's not clear how to construct a suitable reward function, which will need to be a
        function of the robot's sensors. We could try to design a simple reward function that approximately
        captures the intended behavior, but this will often result in behavior that optimizes our reward
        function without actually satisfying our preferences."
    </i></blockquote>
    <p>
        Imagine it's your first time learning to cook. One way we can learn is through <i>imitation</i>, which inspires <i>Imitation 
        Learning</i> (that's a blog for another day). Another way we can learn is through feedback, i.e., we can perform some 
        actions, and a teacher critiques us. This is the essence of PBRL. The agent performs and a human critic evaluates whether 
        its performance is suitable. We can then <b>learn a reward function based off of these human preferences</b> which 
        potentially outperforms suboptimal policies acting upon shortcuts to maximize rewards.
    </p>

    <p>
        <span style="color:#C495C3">&#5463</span> Great, so how do we accomplish this technically? <span style="color:#C495C3">&#5465</span>
    </p>
    <p>
        <i style="color:gray;">As of my knowledge, most PBRL work follow this method.</i>
    </p>
    <p>
        <i style="color:gray;">tl;dr version. </i> First, we must have at least two models: (1) the agent \(\pi_\theta\)
        (2) the reward function estimate \(\hat{r}_\psi\) which estimate the human's <i>unexplainable</i> 
        reward function. We train \(\pi_\theta\) from \(r_t = \hat{r}_\psi(o_t, a_t)\).
        A human compares pairs of collected segments of trajectories \((\sigma^1, \sigma^2)\) by the agent, where \(\sigma = (
        o_1, o_2, ..., o_n)\), and the human reports whether they prefer one over the other (or if they are equal). 
        The reward function estimate is updated based on whether it can accurately capture the human preferences. 
        We can now use this model to feed reward for the agent model.
    </p>
    <p>
        <i style="color:gray;">in-depth version. </i> Since the human gives feedback on their preference regarding 
        trajectory pairs, we must create a method that intuitively shows whether \(\hat{r}_\psi\) also follows the same 
        preference. We use the following <span style="color: #afb696">equation (1)</span> to showcase the reward 
        estimate's preference:
    </p>
    <blockquote style="text-align:center; padding: 15px; background: #afb696; border-radius: 5px;">
        \(
            \begin{array}{c}
            \hat{P}[\sigma^1 > \sigma^2] = \dfrac{\text{exp} (\sum\limits_{t=0}^n \hat{r}(o^1_t, a^1_t))}
            {\text{exp} (\sum\limits_{t=0}^n \hat{r}(o^1_t, a^1_t)) + \text{exp} (\sum\limits_{t=0}^n \hat{r}(o^2_t, a^2_t))}
            \end{array}
        \)
    </blockquote>
    <p>
        This essentially captures the likelihood that the reward function estimate prefers \(\sigma^1\) over \(\sigma^2\). 
        The numerator resembles the exponential of the total sum of rewards from trajectory 1, and the denominator is the sum 
        of the exponential of the total sum of rewards from trajectory 1 combined with the exponential of the total sum of 
        rewards from trajectory 2. <i style="color:gray;">As an example, say in \(\sigma^1\) the agent received a 
        total reward of 10 while in \(\sigma^2\) the agent received a total reward of 5. Then, the likelihood that 
        \(\hat{r}_\psi\) prefers \(\sigma^1\) over \(\sigma^2\) would be \(\frac{\text{exp}(10)}{\text{exp}(10) + 
        \text{exp}(5)} = \frac{e^{10}}{e^{10} + e^5} \approx0.99 \). </i>
    </p>
    <p>
        Now, we use these estimated likelihood preferences and update the model based on the actual labels following 
        this <span style="color:#b497d7">equation (2)</span>:
    </p>
    <blockquote style="text-align:center; padding: 15px; background: #b497d7; border-radius: 5px;">
        \(
            \begin{array}{c}
            \mathcal{L}(\psi) = - \sum\limits_{(\sigma^1, \sigma^2)\in \mathcal{D}} (1-y)\text{ ln }\hat{P}[\sigma^1 > \sigma^2] 
            + y\text{ ln }\hat{P}[\sigma^1 < \sigma^2]
            \end{array}
        \)
    </blockquote>
    <p>
        where \(D\) is the collected human-preference dataset consisting of \((\sigma^1, \sigma^2, y)\) where \(y=(0,1,0.5)\), 
        indiciating the critic's preference (or equal preference). Intuitively, we update the reward function estimate 
        to also have a likelihood preference following \((0,1,0.5)\). Now, we can use cross-entropy loss to update the reward
        function estimate in a Supervised Learning manner. 
    </p>
    <p>
        <i style="color:gray;">Note. Some papers diverge slightly here and there, for example, Kim et al. [4] use 
        the expectation rather than the summation for <span style="color:#b497d7">equation (2)</span>.</i>
    </p>
    <p>
        <span style="color:#C495C3">&#5463</span> Finally, we've accomplished in creating a model that estimates 
        the human's preference. This enables the policy to receive rewards following a complex function that may not 
        be easy to define. <span style="color:#C495C3">&#5465</span>
    </p>

    <p style="font-size:1.2vw; color:#7E91EF">&#13146 Current PBRL advancements</p>
    <p>
        That being said, the results from [6] contradict the outcome we hoped for. While 
        PBRL and traditional RL go head-to-head for the easier, OpenAI Gym tasks, in the more complex, Atari games, 
        the PBRL-trained agent <b>cannot (on-average) outperform the traditional RL agent.</b> Furthermore, even during cases 
        when the PBRL agent successfully outperformed the traditionally-trained agent, the authors use a tremendous number of 
        queries to the human critic - around 5.5k human labels, or in <b>total roughly 5 hrs of human labor</b> (as stated on the 
        footnotes of page 11). 
    </p> 
    <p>
        <span style="color:#7E91EF">&#5463</span> So, what advancements have been made to improve PBRL performance to inch towards our original 
        goal, that is, an agent that learns how to scramble some eggs? <span style="color:#7E91EF">&#5465</span>
    </p>

    <figure style="display:block; margin: auto; width:50%">
        <img src={{.Params.eggs}} style="display:block; margin: auto; width:100%" > 
        <figcaption>egg sammy so yammy (this is lucas sin; he's so dedicated to hong kong cuisine)</figcaption>
    </figure> 

    <p>
        <i style="color:gray;">Imitation Learning + PBRL. </i>
        The next iteration of work published by similar authors to [6] is Ibarz et al.'s work in [7]. Ibarz et al. argue that:
        (1) if only training the agent by preference, the agent may find difficulty in traversing the state space wide enough 
        due to suboptimal comparisons completed by the critic from a pair of poor-performing trajectories (2) training solely 
        by preference is inefficient time-wise. 
    </p>
    <p>
        Hence, the authors decide to pretrain an agent using Imitation Learning and 
        finetune the model using human preferences. While the results of [7] also may not be the best, Ibarz et al. realize that 
        labels outputted by a synthetic oracle aids agent performance significantly. In other words, if the human critic is 
        an expert in the task (and not just some random person hired to participate in research), we can potentally train 
        an agent that develops the optimal policy. <i style="color:gray;">So, then in a task where there is no synthetic 
        oracle and any human is an expert, we can create a super AI! Totally not foreshadowing for a future section...</i>
        
    </p>
    <p>
        <i style="color:gray;">PBRL benchmark.</i> [1] develops a benchmark for PBRL methods. Specifically, they simulate 
        human preferences but add irrationality in these simulations. While the paper goes in-depth 
        in comparing various features, one thing that I'd like to mention specifically is in section 3.2, or this following 
        <span style="color:#c9be4a">equation (3):</span>
    </p>
    <blockquote style="text-align:center; padding: 15px; background: #c9be4a; border-radius: 5px;">
        \(
            \begin{array}{c}
            \hat{P}[\sigma^1 > \sigma^2] = \dfrac{\text{exp} (\beta\sum\limits_{t=0}^n \hat{r}(o^1_t, a^1_t))}
            {\text{exp} (\beta\sum\limits_{t=0}^n \hat{r}(o^1_t, a^1_t)) + \text{exp} (\beta\sum\limits_{t=0}^n \hat{r}(o^2_t, a^2_t))}
            \end{array}
        \)
    </blockquote>
    <p>
        Note that <span style="color:#c9be4a">equation (3)</span> is almost identical to <span style="color:#afb696">
        equation (1)</span>; the only difference stems from this new hyperparameter \(\beta \in [0, \infty)\). The authors in 
        [1] mention that sometimes, human preferences don't stem from rational decision-making (lol). Hence, it's a wise decision to 
        incorporate an irrationality parameter in these simulated decisions to better capture authentic human preferences. 
        The authors play around with \(\beta\); as \(\beta \rightarrow 0\), the more the preferences follow uniform 
        distribution. On the other hand, as \(\beta \rightarrow \infty\), the more the preferences become <i>rational</i> 
        (as any rational human being should prefer a trajectory with higher expected returns) and deterministic.
    </p>

    <p>
        <i style="color:gray;">PBRL + Exploration vs. Exploitation.</i> Lastly, I'll go over [3]. To preface, <i>
        Exploration vs. Exploitation</i> is a classic problem in reinforcement learning. The idea follows that (most probably) 
        there doesn't exist an optimal (and feasible) policy the agent can take that balances exploration (i.e., examining 
        the entire state space) and exploitation (i.e., taking greedy approaches knowing they succeed). Liang et al. summarize 
        that previous research observes better performance when the agent explores the state space based on its history:
    </p>

    <blockquote style="padding: 15px; background: #d7cccc; border-radius: 5px;"><i>
        "Thrun (1992) showed that exploration methods that utilize the agent's history has been shown to
        perform much better than random exploration. Hence, a common setup is to include an intrinsic reward
        as an exploration bonus. The intrinsic reward can be defined by Count-Based methods which
        keep count of previously visited states and rewards the agents for visiting new states Bellemare et al.
        (2016); Tang et al. (2017); Ostrovski et al. (2017)."
    </i></blockquote>

    <p>
        We previously mentioned how an agent might encounter challenges for exploration when we use end-to-end PBRL. Hence, 
        the authors drive an improved exploration behavior by incorporating an intrinsic reward, intuitively based on the 
        human's confidence of feedbacks. The new reward function follows <span style="color:#d8358c">equation (4):</span>
    </p>

    <blockquote style="text-align:center; padding: 15px; background: #d8358c; border-radius: 5px;">
        \(
            \begin{array}{c}
            r := \hat{r}_{\psi}(o_t, a_t) + \beta_t \cdot \hat{r}_{\text{std}}(o_t, a_t)
            \end{array}
        \)
    </blockquote>

    <p>
        where \(\beta_t=\beta_0(1-\rho)^t\) is a decay rate of the instrisic reward as training progresses. The added feature 
        in <span style="color:#d8358c">Equation (4):</span> is \(\hat{r}_{\text{std}}(o_t, a_t)\), which stands for the 
        standard deviation of all the reward functions \(\{\hat{r}_\psi\}^N_{i=1}\). With low confidence of human feedback, 
        comes higher variance between the multitude of reward functions. Therefore, we nudge the agent into exploring such 
        state-action pairs where the critic has less confidence in the expected return. Hyperparameter \(\beta\) serves a 
        similar purpose to \(\epsilon\) in e-greedy methods where over time, the agent should exploit more often than 
        explore. 
    </p>

    <p>
        <i style="color:gray;">
            Note. [2,4] are other papers co-advised by Kimin Lee and Pieter Abbeel that I'd highly checkout.
        </i>
    </p>

    <p style="font-size:1.2vw; color:#F94F4F">&#13147 PBRL for Language Models</p> 

    <p>
        <span style="color:#F94F4F">&#5463</span> 
        We've now seen a basic survey of current PBRL research. Adding on though, one of the most exciting implementations of 
        PBRL is in the training of Large Language Models! 
        <span style="color:#F94F4F">&#5465</span>
    </p>
    <p>
        Generally, LLMs may be trained in an unsupervised learning manner on numerous (huge!) datasets. For example, GPT3 [5]
        use a crazy amount of data to develop a model with commonsense knowledge:
    </p>
    <p style="text-align:center; padding: 15px; background: #7dde8f; border-radius: 5px;">
        \(
            \begin{array}{c c c c}
            \hline
            & \text{Quantity} & \text{Weight in} & \text{Epochs elapsed when} \\
            \text{Dataset} & \text{(tokens)} & \text{training mix} & \text{training for 300B tokens} \\
            \hline
            \text{Common Crawl (filtered)} & \text{410 billion} & \text{60%} & 0.44 \\
            \text{WebText2} & \text{19 billion} & \text{22%} & 2.9 \\
            \text{Books1} & \text{12 billion} & \text{8%} & 1.9 \\
            \text{Books2} & \text{55 billion} & \text{8%} & 0.43 \\
            \text{Wikipedia} & \text{3 billion} & \text{3%} & 3.4 \\
            \end{array}
        \)
    </p>

    <p>
        Alongside this huge dataset, the LLM might get <b>fine-tuned through human-in-the-loop RL</b>! This is where PBRL comes to 
        play. In [8], Wu et al. are tasked to summarize entire books using LLMs, and fine-tune pre-trained models (in their 
        case they use GPT3) through PBRL. The authors compare fine-tuning GPT3 with PBRL vs. Behavior Cloning on expert 
        demonstrations and observe that PBRL outperforms when data increases. 
    </p>

    <p>
        The authors also explain how researchers must consider <i>scalable oversight</i>, i.e. developing an effective 
        training signal when tasks scale larger. A task such as summarizing books may cause challenges for researchers to 
        design a straightforward training signal for the LLM to perform effectively. <b>Potentially, the only path to 
        providing a beneficial training signal is to use PBRL.</b> This provides valuable insight into fine-tuning other LLMs 
        (or just large models) for a variety of tasks that we aim to solve in the future (such as scrambling an egg!). While 
        PBRL must have access to a laborous amount of data, the benefit it may provide (alongside the fact that it's magnitudes
        smaller than the datasets used) for model performance brings excitement into each step taken closer towards AGI.
    </p>
    
    <p style="font-size:1.2vw; color:#c0b111">&#5133 References</p> 
    [1] Lee et al. B-Pref: Benchmarking Preference-Based Reinforcement Learning. NeurIPS 2021. <br>
    [2] Park et al. SURF: SEMI-SUPERVISED REWARD LEARNING WITH DATA AUGMENTATION FOR FEEDBACK-EFFICIENT  PREFERENCE-BASED REINFORCEMENT LEARNING. ICLR 2022. <br>
    [3] Liang et al. REWARD UNCERTAINTY FOR EXPLORATION IN PREFERENCE-BASED REINFORCEMENT LEARNING. ICLR 2022. <br>
    [4] Kim et al. PREFERENCE TRANSFORMER: MODELING HUMAN PREFERENCES USING TRANSFORMERS FOR RL. ICLR 2023. <br>
    [5] Brown et al. Language Models are Few-Shot Learners. OpenAI 2020. <br>
    [6] Christiano et al. Deep Reinforcement Learning from Human Preferences. NIPS 2017. <br>
    [7] Ibarz et al. Reward learning from human preferences and demonstrations in Atari. NIPS 2018. <br>
    [8] Wu et al. Recursively Summarizing Books with Human Feedback. OpenAI 2021. <br>

</body>
{{ end }}